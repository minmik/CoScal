F8131:/data/local/tmp # cat /sys/class/kgsl/kgsl-3d0/devfreq/governor                                                                                                                           
powersave
/benchmark_model --graph=./lite-model_aiy_vision_classifier_food_V1_1.tflite --use_gpu=true --allow_fp16=true --gpu_backend=cl                                                                 <
STARTING!
Duplicate flags: num_threads
Min num runs: [50]
Min runs duration (seconds): [1]
Max runs duration (seconds): [150]
Inter-run delay (seconds): [-1]
Num threads: [1]
Use caching: [0]
Benchmark name: []
Output prefix: []
Min warmup runs: [1]
Min warmup runs duration (seconds): [0.5]
Graph: [./lite-model_aiy_vision_classifier_food_V1_1.tflite]
Input layers: []
Input shapes: []
Input value ranges: []
Input layer values files: []
Use legacy nnapi : [0]
Allow fp16 : [1]
Require full delegation : [0]
Enable op profiling: [0]
Max profiling buffer entries: [1024]
CSV File to export profiling data to: []
Enable platform-wide tracing: [0]
#threads used for CPU inference: [1]
Max number of delegated partitions : [0]
Min nodes per partition : [0]
External delegate path : []
External delegate options : []
Use gpu : [1]
Allow lower precision in gpu : [1]
Enable running quant models in gpu : [1]
GPU backend : [cl]
Use Hexagon : [0]
Hexagon lib path : [/data/local/tmp]
Hexagon Profiling : [0]
Use nnapi : [0]
Use xnnpack : [0]
Loaded model ./lite-model_aiy_vision_classifier_food_V1_1.tflite
INFO: Initialized TensorFlow Lite runtime.
INFO: Created TensorFlow Lite delegate for GPU.
INFO: Initialized OpenCL-based API.
INFO: Created 1 GPU delegate kernels.
Applied GPU delegate, and the model graph will be completely executed w/ the delegate.
The input model file size (MB): 21.1516
Initialized session in 1570.63ms.
Running benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.
count=34 first=16948 curr=19533 min=9328 max=19533 avg=14770.3 std=2754

Running benchmark for at least 50 iterations and at least 1 seconds but terminate if exceeding 150 seconds.
count=66 first=15896 curr=13837 min=9805 max=18013 avg=15242.2 std=1944

Inference timings in us: Init: 1570631, First inference: 16948, Warmup (avg): 14770.3, Inference (avg): 15242.2
Note: as the benchmark tool itself affects memory footprint, the following is only APPROXIMATE to the actual memory footprint of the model at runtime. Take the information at your discretion.
Peak memory footprint (MB): init=78.2773 overall=78.2773
F8131:/data/local/tmp # cat /sys/class/kgsl/kgsl-3d0/devfreq/governor                                                                                                                           
performance
/benchmark_model --graph=./lite-model_aiy_vision_classifier_food_V1_1.tflite --use_gpu=true --allow_fp16=true --gpu_backend=cl                                                                 <
STARTING!
Duplicate flags: num_threads
Min num runs: [50]
Min runs duration (seconds): [1]
Max runs duration (seconds): [150]
Inter-run delay (seconds): [-1]
Num threads: [1]
Use caching: [0]
Benchmark name: []
Output prefix: []
Min warmup runs: [1]
Min warmup runs duration (seconds): [0.5]
Graph: [./lite-model_aiy_vision_classifier_food_V1_1.tflite]
Input layers: []
Input shapes: []
Input value ranges: []
Input layer values files: []
Use legacy nnapi : [0]
Allow fp16 : [1]
Require full delegation : [0]
Enable op profiling: [0]
Max profiling buffer entries: [1024]
CSV File to export profiling data to: []
Enable platform-wide tracing: [0]
#threads used for CPU inference: [1]
Max number of delegated partitions : [0]
Min nodes per partition : [0]
External delegate path : []
External delegate options : []
Use gpu : [1]
Allow lower precision in gpu : [1]
Enable running quant models in gpu : [1]
GPU backend : [cl]
Use Hexagon : [0]
Hexagon lib path : [/data/local/tmp]
Hexagon Profiling : [0]
Use nnapi : [0]
Use xnnpack : [0]
Loaded model ./lite-model_aiy_vision_classifier_food_V1_1.tflite
INFO: Initialized TensorFlow Lite runtime.
INFO: Created TensorFlow Lite delegate for GPU.
INFO: Initialized OpenCL-based API.
INFO: Created 1 GPU delegate kernels.
Applied GPU delegate, and the model graph will be completely executed w/ the delegate.
The input model file size (MB): 21.1516
Initialized session in 1554.87ms.
Running benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.
count=34 first=11888 curr=16185 min=10114 max=18518 avg=15034.7 std=2372

Running benchmark for at least 50 iterations and at least 1 seconds but terminate if exceeding 150 seconds.
count=63 first=16559 curr=15519 min=15260 max=18146 avg=15979.6 std=489

Inference timings in us: Init: 1554872, First inference: 11888, Warmup (avg): 15034.7, Inference (avg): 15979.6
Note: as the benchmark tool itself affects memory footprint, the following is only APPROXIMATE to the actual memory footprint of the model at runtime. Take the information at your discretion.
Peak memory footprint (MB): init=78.2695 overall=78.2695

